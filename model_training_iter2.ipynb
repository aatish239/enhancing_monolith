{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import mean_squared_error, precision_score, recall_score, accuracy_score,r2_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
      ],
      "metadata": {
        "id": "aTi8jyBalToA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_loc = r\"drive/MyDrive/MSUoA/merged_dataset.csv\"\n",
        "merged_df = pd.read_csv(merged_df_loc, index_col=0)"
      ],
      "metadata": {
        "id": "bchrVUZXlahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-e-hq2kmG_",
        "outputId": "955874d3-a2cc-4d97-8de6-0bf956256e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-561ec325a47c>:20: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
            "  return np.fromstring(x.strip('[]'), sep=' ')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, MinMaxScaler\n",
        "import ast\n",
        "\n",
        "# Safely convert string lists to Python lists using ast.literal_eval\n",
        "def safe_eval_list(x):\n",
        "    if isinstance(x, str):\n",
        "        return ast.literal_eval(x)\n",
        "    return x\n",
        "\n",
        "# Convert string lists into proper lists\n",
        "merged_df['genres'] = merged_df['genres'].apply(safe_eval_list)\n",
        "merged_df['directors_list'] = merged_df['directors_list'].apply(safe_eval_list)\n",
        "merged_df['writers_list'] = merged_df['writers_list'].apply(safe_eval_list)\n",
        "\n",
        "# Convert string representations of embeddings to numpy arrays\n",
        "def convert_string_to_array(x):\n",
        "    if isinstance(x, str):\n",
        "        return np.fromstring(x.strip('[]'), sep=' ')\n",
        "    return np.array(x)\n",
        "\n",
        "# Apply conversion across the DataFrame\n",
        "merged_df['user_reviews_padded'] = merged_df['user_reviews_padded'].apply(convert_string_to_array)\n",
        "\n",
        "# Encode categorical variables\n",
        "mlb_genres = MultiLabelBinarizer()\n",
        "mlb_writers = MultiLabelBinarizer()\n",
        "mlb_directors = MultiLabelBinarizer()\n",
        "\n",
        "genres_encoded = mlb_genres.fit_transform(merged_df['genres'])\n",
        "writers_encoded = mlb_writers.fit_transform(merged_df['writers_list'])\n",
        "directors_encoded = mlb_directors.fit_transform(merged_df['directors_list'])\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler_numeric = MinMaxScaler()\n",
        "numeric_features = scaler_numeric.fit_transform(merged_df[['imdb_rating', 'numVotes']])\n",
        "\n",
        "# Consistent user_reviews_padded embeddings\n",
        "embedding_dim = len(merged_df['user_reviews_padded'].iloc[0])\n",
        "merged_df['user_reviews_padded'] = merged_df['user_reviews_padded'].apply(\n",
        "    lambda x: x if len(x) == embedding_dim else np.zeros(embedding_dim)\n",
        ")\n",
        "\n",
        "# Concatenate all features into X\n",
        "X = np.concatenate([\n",
        "    np.vstack(merged_df['user_reviews_padded'].values),\n",
        "    numeric_features,\n",
        "    genres_encoded,\n",
        "    writers_encoded,\n",
        "    directors_encoded\n",
        "], axis=1)\n",
        "\n",
        "# Define y (scaled user ratings)\n",
        "y = merged_df['user_ratings'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Encode user IDs and movie IDs\n",
        "user_encoder = LabelEncoder()\n",
        "movie_encoder = LabelEncoder()\n",
        "merged_df['user_id_encoded'] = user_encoder.fit_transform(merged_df['user_id'])\n",
        "merged_df['movie_id_encoded'] = movie_encoder.fit_transform(merged_df['movie_ids'])\n",
        "\n",
        "# Train-test split\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten() # y_scaled will be same because user_ratings are already scaled\n",
        "\n",
        "# Splitting both features and movie IDs\n",
        "X_train, X_test, y_train, y_test, movie_train, movie_test = train_test_split(\n",
        "    X, y_scaled, merged_df['movie_id_encoded'].values, test_size=0.25, random_state=42\n",
        ")\n",
        "# Check the shapes of training data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"movie_train shape:\", movie_train.shape)\n",
        "\n",
        "movie_input = Input(shape=(1,), name='movie_input')\n",
        "movie_embedding = Embedding(input_dim=len(movie_encoder.classes_), output_dim=50, name='movie_embedding')(movie_input)\n",
        "movie_vec = Flatten(name='movie_flatten')(movie_embedding)\n",
        "\n",
        "# Contextual Features Input\n",
        "context_input = Input(shape=(X_train.shape[1],), name='context_input')\n",
        "\n",
        "# Concatenate movie embedding with context input\n",
        "concat = Concatenate()([movie_vec, context_input])\n",
        "\n",
        "# Dense Layers for cold-start movie recommendation\n",
        "x = Dense(512, activation='relu')(concat)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Define Cold Start Movie Recommendation Model\n",
        "cold_start_movie_model = Model(inputs=[movie_input, context_input], outputs=output)\n",
        "cold_start_movie_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Model summary\n",
        "print(cold_start_movie_model.summary())\n",
        "\n",
        "# Train the model\n",
        "history_cold_start = cold_start_movie_model.fit(\n",
        "    [movie_train, X_train], y_train,  # Using only training data\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "0e-LutMKkrlj",
        "outputId": "f8505f1c-e0fc-489d-b191-8bcde10850c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (31020, 25639)\n",
            "movie_train shape: (31020,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ movie_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m810,750\u001b[0m │ movie_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ movie_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25639\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25689\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ movie_flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ context_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m13,153,280\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m131,328\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ movie_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">810,750</span> │ movie_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ movie_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25639</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25689</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ movie_flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ context_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,153,280</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,128,383\u001b[0m (53.90 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,128,383</span> (53.90 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,128,383\u001b[0m (53.90 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,128,383</span> (53.90 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0914 - mae: 0.2330 - val_loss: 0.3459 - val_mae: 0.2083\n",
            "Epoch 2/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0794 - mae: 0.1815 - val_loss: 0.0840 - val_mae: 0.1972\n",
            "Epoch 3/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1478 - val_loss: 0.0677 - val_mae: 0.1975\n",
            "Epoch 4/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0478 - mae: 0.1339 - val_loss: 0.0799 - val_mae: 0.1967\n",
            "Epoch 5/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0315 - mae: 0.1232 - val_loss: 0.0650 - val_mae: 0.1950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model compression\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the path to save the converted TFLite model\n",
        "tflite_model_path = \"cold_start_movie_model.tflite\"\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(cold_start_movie_model)\n",
        "\n",
        "# Apply optimization (quantization for smaller size and faster inference)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "with open(tflite_model_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"✅ Model successfully converted and saved as {tflite_model_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN-VitFlBo21",
        "outputId": "611ae4cb-6891-4c7b-c1ca-6ed07050310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp7uqq9fbg'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 1), dtype=tf.float32, name='movie_input'), TensorSpec(shape=(None, 25639), dtype=tf.float32, name='context_input')]\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137633200945104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633200945296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633200945872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633200946064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633200945680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633193083728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633193085264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633193084496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137633193086608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ Model successfully converted and saved as cold_start_movie_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting preds for the compressed model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"cold_start_movie_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input & output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Print expected input shapes for debugging\n",
        "print(f\"🔍 Expected Input 0 Shape: {input_details[0]['shape']}, Type: {input_details[0]['dtype']}\")\n",
        "print(f\"🔍 Expected Input 1 Shape: {input_details[1]['shape']}, Type: {input_details[1]['dtype']}\")\n",
        "\n",
        "# Function to run inference with TFLite model\n",
        "def predict_tflite(context_feature, movie_embedding):\n",
        "    \"\"\"Runs inference using the TensorFlow Lite model with correctly shaped inputs.\"\"\"\n",
        "    # Convert inputs to FLOAT32 and match expected shapes\n",
        "    context_feature = np.array(context_feature, dtype=np.float32).reshape(1, 1)  # Ensure (1, 1) shape\n",
        "    movie_embedding = np.array(movie_embedding, dtype=np.float32).reshape(1, -1)  # Ensure (1, 25639) shape\n",
        "\n",
        "    # Ensure input shapes match model expectations\n",
        "    assert movie_embedding.shape[1] == input_details[0]['shape'][1], f\"Mismatch in movie_embedding shape: {movie_embedding.shape}\"\n",
        "    assert context_feature.shape[1] == input_details[1]['shape'][1], f\"Mismatch in context_feature shape: {context_feature.shape}\"\n",
        "\n",
        "    # Set model inputs\n",
        "    interpreter.set_tensor(input_details[0]['index'], movie_embedding)\n",
        "    interpreter.set_tensor(input_details[1]['index'], context_feature)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get output prediction\n",
        "    return interpreter.get_tensor(output_details[0]['index'])[0][0]  # Return scalar prediction\n",
        "\n",
        "# Test the model with a sample (use 10 test samples)\n",
        "sample_movie_embeddings = X_test[:10]  # Movie embeddings (25639 features each)\n",
        "sample_context_features = movie_test[:10].reshape(-1, 1)  # Context features (should be 1 per movie)\n",
        "\n",
        "# Get TFLite model predictions for each sample (one-by-one)\n",
        "tflite_predictions = [predict_tflite(context_feature, movie_embedding)\n",
        "                      for context_feature, movie_embedding in zip(sample_context_features, sample_movie_embeddings)]\n",
        "\n",
        "# Convert to NumPy array\n",
        "tflite_predictions = np.array(tflite_predictions)\n",
        "\n",
        "print(\"✅ TFLite Model Inference Completed. Sample Predictions:\", tflite_predictions[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR-ZdYVNB0R-",
        "outputId": "845a2b47-b9e1-43a8-de3f-9a12d80227e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Expected Input 0 Shape: [    1 25639], Type: <class 'numpy.float32'>\n",
            "🔍 Expected Input 1 Shape: [1 1], Type: <class 'numpy.float32'>\n",
            "✅ TFLite Model Inference Completed. Sample Predictions: [0.62075853 0.5239425  0.63836324 0.738609   0.7438443 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute rmse\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Get true ratings\n",
        "y_true = y_test[:10]  # Use same test samples\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_true, tflite_predictions))\n",
        "print(f\" RMSE (TFLite Model): {rmse:.4f}\")\n",
        "\n",
        "def precision_at_k(y_true, y_pred, k=5):\n",
        "    \"\"\"Computes Precision@K for Top-K recommendations.\"\"\"\n",
        "    top_k_preds = np.argsort(y_pred)[::-1][:k]  # Get top-K predicted indices\n",
        "    relevant_items = set(np.argsort(y_true)[::-1][:k])  # Get top-K actual relevant indices\n",
        "    return len(set(top_k_preds) & relevant_items) / k\n",
        "\n",
        "def recall_at_k(y_true, y_pred, k=5):\n",
        "    \"\"\"Computes Recall@K for Top-K recommendations.\"\"\"\n",
        "    top_k_preds = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = set(np.argsort(y_true)[::-1][:k])\n",
        "    return len(set(top_k_preds) & relevant_items) / len(relevant_items)\n",
        "\n",
        "# Compute Precision@K and Recall@K\n",
        "k = 5\n",
        "precision_k = precision_at_k(y_true, tflite_predictions, k)\n",
        "recall_k = recall_at_k(y_true, tflite_predictions, k)\n",
        "\n",
        "print(f\"📊 Precision@{k}: {precision_k:.4f}\")\n",
        "print(f\"📊 Recall@{k}: {recall_k:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPR7sowZDAwL",
        "outputId": "1ca78ee2-9f86-4cdf-8790-79291d0e2ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RMSE (TFLite Model): 0.3563\n",
            "📊 Precision@5: 0.6000\n",
            "📊 Recall@5: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Compressed model size\\n\" , os.stat(\"cold_start_movie_model.tflite\").st_size / (1024 * 1024))\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Save the original model as an H5 file\n",
        "original_model_path = \"cold_start_movie_model.h5\"\n",
        "cold_start_movie_model.save(original_model_path)\n",
        "\n",
        "print(f\" Original model saved as {original_model_path}\")\n",
        "print(os.stat(\"cold_start_movie_model.h5\").st_size / (1024 * 1024))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7ZU6H2BBsrA",
        "outputId": "10e99e00-9e89-42e2-f910-eac5b99b635e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressed model size\n",
            " 13.491241455078125\n",
            " Original model saved as cold_start_movie_model.h5\n",
            "161.73059844970703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSnkdJE2Y2m",
        "outputId": "f4456b3e-d5b6-4fa1-abbd-44c9858451e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def manual_minmax_descale(scaled_ratings, original_min=1, original_max=10):\n",
        "    \"\"\"\n",
        "    Converts MinMax scaled ratings back to their original scale.\n",
        "\n",
        "    Args:\n",
        "        scaled_ratings (np.array): The scaled ratings between 0 and 1.\n",
        "        original_min (float): The minimum value of the original scale.\n",
        "        original_max (float): The maximum value of the original scale.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Descaled ratings in the original scale.\n",
        "    \"\"\"\n",
        "    return scaled_ratings * (original_max - original_min) + original_min\n",
        "\n",
        "\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Function to Calculate Precision@K\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Precision@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]  # Get top-k indices\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()  # Count relevant hits\n",
        "    return relevant / k\n",
        "\n",
        "# Function to Calculate Recall@K\n",
        "def recall_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Recall@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]\n",
        "    total_relevant = np.sum(y_true)\n",
        "    if total_relevant == 0:\n",
        "        return 0.0  # Avoid division by zero\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()\n",
        "    return relevant / total_relevant\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = cold_start_movie_model.predict([movie_test, X_test]).flatten()\n",
        "\n",
        "# Calculate RMSE\n",
        "y_test_original = y_test.reshape(-1, 1).flatten()\n",
        "rmse_score = calculate_rmse(y_test_original, predictions)\n",
        "print(\"RMSE:\", rmse_score)\n",
        "\n",
        "# Convert True Ratings to Binary Relevance (e.g., ratings >= 4.0 are relevant)\n",
        "y_true_binary = (y_test_original >= 0.4).astype(int)  # Adjust threshold since ratings are scaled between 0-1\n",
        "\n",
        "# Calculate Precision@K and Recall@K for K=10\n",
        "k = 10\n",
        "k = min(k, len(y_true_binary))  # Adjust K if necessary\n",
        "\n",
        "precision = precision_at_k(y_true_binary, predictions, k)\n",
        "recall = recall_at_k(y_true_binary, predictions, k)\n",
        "\n",
        "# Print Metrics\n",
        "print(f\"Precision@{k}: {precision:.4f}\")\n",
        "print(f\"Recall@{k}: {recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecU3iLD10gaz",
        "outputId": "c0c97394-5896-433d-e06c-8390b278166d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "RMSE: 0.25502380039151223\n",
            "Precision@10: 1.0000\n",
            "Recall@10: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Do These Results Indicate?\n",
        "RMSE = 0.2649 (on scaled ratings)\n",
        "\n",
        "Since your ratings were scaled between [0, 1], this corresponds to approximately:\n",
        "0.2649\n",
        "×\n",
        "(\n",
        "10\n",
        "−\n",
        "1\n",
        ")\n",
        "≈\n",
        "2.38\n",
        "0.2649×(10−1)≈2.38\n",
        "A descaled RMSE of ~2.38 suggests that your model predicts fairly well but has some room for improvement.\n",
        "Precision@10 = 0.9000\n",
        "\n",
        "Out of the top 10 recommendations, 9 out of 10 movies are relevant (i.e., ratings ≥ 4.0).\n",
        "This is a great result! It means your top recommendations are highly accurate.\n",
        "Recall@10 = 0.0042\n",
        "\n",
        "Only 0.42% of all relevant movies are being captured in the top 10 recommendations.\n",
        "This suggests that while your model is good at recommending a few relevant movies (high precision), it misses most of the relevant movies (low recall)."
      ],
      "metadata": {
        "id": "EcHA1Zcf1CVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE table\n",
        "\n",
        "0 - 1.0\tExcellent (almost perfect predictions)\n",
        "\n",
        "1.0 - 2.0\tGood (strong predictive power)\n",
        "\n",
        "2.0 - 3.0\tFair (room for improvement)\n",
        "\n",
        "greater than 3.0\tPoor (model is not performing well)\n"
      ],
      "metadata": {
        "id": "t9NvlOLfv4mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Improve RMSE Further\n",
        "Here are some strategies to potentially improve your RMSE:\n",
        "\n",
        "Feature Engineering:\n",
        "Add more contextual features (e.g., release year, cast, user demographics).\n",
        "\n",
        "Model Improvements:\n",
        "Use deeper neural networks or advanced architectures like Neural Collaborative Filtering (NCF).\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "Tune learning rates, batch sizes, or layer sizes.\n",
        "\n",
        "Increase Epochs:\n",
        "Train for more epochs with early stopping to prevent overfitting.\n",
        "\n",
        "Regularization Techniques:\n",
        "Add L2 regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "44iImUhXvq3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from movie_id_encoded to primaryTitle\n",
        "movie_id_to_title = dict(zip(merged_df['movie_ids'], merged_df['primaryTitle']))\n",
        "\n",
        "# Generate binary encodings for genres, writers, and directors based on user preferences\n",
        "def get_cold_start_context(fav_genres, fav_writers, fav_directors):\n",
        "    # Encode genres, writers, and directors\n",
        "    genre_vector = mlb_genres.transform([fav_genres]) if fav_genres else np.zeros((1, len(mlb_genres.classes_)))\n",
        "    writer_vector = mlb_writers.transform([fav_writers]) if fav_writers else np.zeros((1, len(mlb_writers.classes_)))\n",
        "    director_vector = mlb_directors.transform([fav_directors]) if fav_directors else np.zeros((1, len(mlb_directors.classes_)))\n",
        "\n",
        "    # Generate dummy numerical features (imdb_rating and numVotes)\n",
        "    dummy_numerical_features = np.zeros((1, 2))  # Placeholder for numerical features\n",
        "\n",
        "    # Generate dummy user review embeddings\n",
        "    # embedding_dim = 17543 - (2 + len(mlb_genres.classes_) + len(mlb_writers.classes_) + len(mlb_directors.classes_))\n",
        "    # Dynamically determine embedding_dim based on available user review embeddings\n",
        "    if isinstance(merged_df['user_reviews_padded'].iloc[0], np.ndarray):\n",
        "        embedding_dim = merged_df['user_reviews_padded'].iloc[0].shape[0]\n",
        "    else:\n",
        "        embedding_dim = 768  # Default to BERT embedding size\n",
        "    dummy_user_reviews = np.zeros((1, embedding_dim))\n",
        "\n",
        "    # Concatenate all features to match the expected input size\n",
        "    context_vector = np.concatenate([\n",
        "        dummy_user_reviews,\n",
        "        dummy_numerical_features,\n",
        "        genre_vector,\n",
        "        writer_vector,\n",
        "        director_vector\n",
        "    ], axis=1)\n",
        "\n",
        "    return context_vector\n",
        "\n",
        "\n",
        "# Function to recommend movies for cold-start users\n",
        "def recommend_cold_start_movies(fav_genres=None, fav_writers=None, fav_directors=None, top_n=10):\n",
        "    # Generate movie indices and ensure proper dtype\n",
        "    all_movies = np.arange(len(movie_encoder.classes_)).reshape(-1, 1).astype(np.float32)  # Convert to float32\n",
        "\n",
        "    # Generate context vector based on user preferences\n",
        "    context_features = get_cold_start_context(fav_genres, fav_writers, fav_directors)\n",
        "\n",
        "    # Repeat context for all movies and ensure proper dtype\n",
        "    context_repeated = np.tile(context_features, (len(all_movies), 1)).astype(np.float32)  # Convert to float32\n",
        "\n",
        "    # Check input shapes and types before prediction\n",
        "    print(\"all_movies shape:\", all_movies.shape, all_movies.dtype)\n",
        "    print(\"context_repeated shape:\", context_repeated.shape, context_repeated.dtype)\n",
        "\n",
        "    # Predict ratings for all movies\n",
        "    predictions = cold_start_movie_model.predict([all_movies, context_repeated]).flatten()\n",
        "\n",
        "    # Descend the predicted ratings back to the original scale (1-10)\n",
        "    descaled_predictions = manual_minmax_descale(predictions)\n",
        "\n",
        "    # Retrieve top N movie indices\n",
        "    top_indices = descaled_predictions.argsort()[-top_n:][::-1]\n",
        "    recommended_movie_ids = movie_encoder.inverse_transform(top_indices)\n",
        "\n",
        "    # Map movie IDs to movie titles\n",
        "    recommended_movies = [movie_id_to_title.get(movie_id, \"Unknown Movie\") for movie_id in recommended_movie_ids]\n",
        "\n",
        "    # Return the list of recommended movies with predicted ratings\n",
        "    return [(title, descaled_predictions[i]) for i, title in zip(top_indices, recommended_movies)]\n",
        "\n"
      ],
      "metadata": {
        "id": "bhH3FPxewV5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommend top 5 movies for a cold-start user with provided preferences\n",
        "top_cold_start_movies = recommend_cold_start_movies(\n",
        "    fav_genres=['Action', 'Drama'],\n",
        "    fav_writers=['nm0522871'],\n",
        "    fav_directors=['nm0412650'],\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "# Print the recommendations\n",
        "print(\"Top 5 Cold-Start Movie Recommendations:\")\n",
        "for movie, score in top_cold_start_movies:\n",
        "    print(f\"Movie Title: {movie}, Predicted Rating: {score:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIWHbxx9wYnU",
        "outputId": "3a244807-b9ce-4e46-95ec-f689da1fdc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_movies shape: (16215, 1) float32\n",
            "context_repeated shape: (16215, 25639) float32\n",
            "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Top 5 Cold-Start Movie Recommendations:\n",
            "Movie Title: Shree 420, Predicted Rating: 8.37\n",
            "Movie Title: Frühling auf dem Eis, Predicted Rating: 8.37\n",
            "Movie Title: Johnny Come Lately, Predicted Rating: 8.30\n",
            "Movie Title: The Big Gamble, Predicted Rating: 8.23\n",
            "Movie Title: The Pad (and How to Use It), Predicted Rating: 8.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1oaGTfp5y1p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Encode user IDs\n",
        "user_encoder = LabelEncoder()\n",
        "merged_df['user_id_encoded'] = user_encoder.fit_transform(merged_df['user_id'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test, user_train, user_test = train_test_split(\n",
        "    X, y_scaled, merged_df['user_id_encoded'].values, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Check data shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"user_train shape:\", user_train.shape)\n",
        "\n",
        "# User Input and Embedding\n",
        "user_input = Input(shape=(1,), name='user_input')\n",
        "user_embedding = Embedding(input_dim=len(user_encoder.classes_), output_dim=50, name='user_embedding')(user_input)\n",
        "user_vec = Flatten(name='user_flatten')(user_embedding)\n",
        "\n",
        "# Movie Context Input\n",
        "movie_context_input = Input(shape=(X_train.shape[1],), name='movie_context_input')\n",
        "\n",
        "# Concatenate user embedding with movie context\n",
        "concat_user_movie = Concatenate()([user_vec, movie_context_input])\n",
        "\n",
        "# Dense Layers\n",
        "x = Dense(512, activation='relu')(concat_user_movie)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Define Top-N User Recommendation Model\n",
        "top_n_user_model = Model(inputs=[user_input, movie_context_input], outputs=output)\n",
        "top_n_user_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Model summary\n",
        "print(top_n_user_model.summary())\n",
        "\n",
        "# Train the model\n",
        "history = top_n_user_model.fit(\n",
        "    [user_train, X_train], y_train,  # Using only training data\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "O8YgY5JplBhS",
        "outputId": "8e4019c4-2c7c-4258-8926-46a9de494ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (31020, 25639)\n",
            "user_train shape: (31020,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ user_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ user_embedding            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │      \u001b[38;5;34m1,328,850\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ user_flatten (\u001b[38;5;33mFlatten\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ user_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_context_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25639\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25689\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ user_flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ movie_context_input[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m13,153,280\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m131,328\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m32,896\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ user_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ user_embedding            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,328,850</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ user_flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ movie_context_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25639</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25689</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ movie_context_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,153,280</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,646,483\u001b[0m (55.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,483</span> (55.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,646,483\u001b[0m (55.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,646,483</span> (55.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.1007 - mae: 0.2417 - val_loss: 0.0998 - val_mae: 0.2054\n",
            "Epoch 2/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0637 - mae: 0.1741 - val_loss: 0.0921 - val_mae: 0.1982\n",
            "Epoch 3/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0428 - mae: 0.1357 - val_loss: 0.0765 - val_mae: 0.2042\n",
            "Epoch 4/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0246 - mae: 0.1082 - val_loss: 0.0645 - val_mae: 0.1948\n",
            "Epoch 5/5\n",
            "\u001b[1m388/388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0236 - mae: 0.0925 - val_loss: 0.0635 - val_mae: 0.1951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Save the original Keras model\n",
        "original_model_path = \"top_n_user_model.keras\"\n",
        "top_n_user_model.save(original_model_path)\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "tflite_model_path = \"top_n_user_model.tflite\"\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(top_n_user_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Apply quantization\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the compressed model\n",
        "with open(tflite_model_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"✅ TFLite Model saved at: {tflite_model_path}\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Get file sizes\n",
        "original_size = os.path.getsize(original_model_path) / (1024 * 1024)  # Convert bytes to MB\n",
        "tflite_size = os.path.getsize(tflite_model_path) / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "# Print comparison\n",
        "print(f\"📏 Model Size Comparison:\")\n",
        "print(f\"🔹 Original Model Size: {original_size:.2f} MB\")\n",
        "print(f\"🔹 Compressed TFLite Model Size: {tflite_size:.2f} MB\")\n",
        "print(f\"🔹 Compression Ratio: {(tflite_size / original_size) * 100:.2f}%\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"top_n_user_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input & output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Print expected input shapes for debugging\n",
        "print(f\"🔍 Expected User Input Shape: {input_details[0]['shape']}, Type: {input_details[0]['dtype']}\")\n",
        "print(f\"🔍 Expected Movie Context Input Shape: {input_details[1]['shape']}, Type: {input_details[1]['dtype']}\")\n",
        "\n",
        "# Function to run inference with TFLite model (FIXED)\n",
        "def predict_tflite(movie_context, user_id):\n",
        "    \"\"\"Runs inference using the TensorFlow Lite model with correctly shaped inputs.\"\"\"\n",
        "    # Convert inputs to FLOAT32 and match expected shapes\n",
        "    movie_context = np.array(movie_context, dtype=np.float32).reshape(1, 25639)  # Ensure (1, 25639) shape\n",
        "    user_id = np.array(user_id, dtype=np.float32).reshape(1, 1)  # Ensure (1, 1) shape\n",
        "\n",
        "    # Ensure input shapes match model expectations\n",
        "    assert movie_context.shape[1] == input_details[0]['shape'][1], f\"Mismatch in movie_context shape: {movie_context.shape}\"\n",
        "    assert user_id.shape[1] == input_details[1]['shape'][1], f\"Mismatch in user_id shape: {user_id.shape}\"\n",
        "\n",
        "    # Set model inputs\n",
        "    interpreter.set_tensor(input_details[0]['index'], movie_context)\n",
        "    interpreter.set_tensor(input_details[1]['index'], user_id)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get output prediction\n",
        "    return interpreter.get_tensor(output_details[0]['index'])[0][0]  # Return scalar prediction\n",
        "\n",
        "# Test the model with a sample (use 10 test samples)\n",
        "num_samples = 10\n",
        "movie_context_samples = X_test[:num_samples]  # Movie embeddings (25639 features each)\n",
        "user_samples = user_test[:num_samples].reshape(-1, 1)  # User IDs (1 per sample)\n",
        "\n",
        "# Get TFLite model predictions for each sample (one-by-one)\n",
        "tflite_predictions = [predict_tflite(movie_context, user_id)\n",
        "                      for movie_context, user_id in zip(movie_context_samples, user_samples)]\n",
        "\n",
        "# Convert to NumPy array\n",
        "tflite_predictions = np.array(tflite_predictions)\n",
        "\n",
        "print(\"✅ TFLite Model Inference Completed. Sample Predictions:\", tflite_predictions[:5])\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Get true ratings for test samples\n",
        "y_true = y_test[:num_samples]\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_true, tflite_predictions))\n",
        "print(f\"📊 RMSE (TFLite Model): {rmse:.4f}\")\n",
        "\n",
        "def precision_at_k(y_true, y_pred, k=5):\n",
        "    \"\"\"Computes Precision@K for Top-K recommendations.\"\"\"\n",
        "    top_k_preds = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = set(np.argsort(y_true)[::-1][:k])\n",
        "    return len(set(top_k_preds) & relevant_items) / k\n",
        "\n",
        "def recall_at_k(y_true, y_pred, k=5):\n",
        "    \"\"\"Computes Recall@K for Top-K recommendations.\"\"\"\n",
        "    top_k_preds = np.argsort(y_pred)[::-1][:k]\n",
        "    relevant_items = set(np.argsort(y_true)[::-1][:k])\n",
        "    return len(set(top_k_preds) & relevant_items) / len(relevant_items)\n",
        "\n",
        "k = 5\n",
        "precision_k = precision_at_k(y_true, tflite_predictions, k)\n",
        "recall_k = recall_at_k(y_true, tflite_predictions, k)\n",
        "\n",
        "print(f\"📊 Precision@{k}: {precision_k:.4f}\")\n",
        "print(f\"📊 Recall@{k}: {recall_k:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5pmjA87Fp4n",
        "outputId": "baa2e999-5423-4059-b6d0-a9dad9fc6bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp85x3lky1'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 1), dtype=tf.float32, name='user_input'), TensorSpec(shape=(None, 25639), dtype=tf.float32, name='movie_context_input')]\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137603912270288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912270480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912271440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912271056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912273360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912272592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912274512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912273936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137603912275280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ TFLite Model saved at: top_n_user_model.tflite\n",
            "📏 Model Size Comparison:\n",
            "🔹 Original Model Size: 167.66 MB\n",
            "🔹 Compressed TFLite Model Size: 13.99 MB\n",
            "🔹 Compression Ratio: 8.34%\n",
            "🔍 Expected User Input Shape: [    1 25639], Type: <class 'numpy.float32'>\n",
            "🔍 Expected Movie Context Input Shape: [1 1], Type: <class 'numpy.float32'>\n",
            "✅ TFLite Model Inference Completed. Sample Predictions: [0.6394081  0.53195035 0.6176715  0.7182077  0.64469045]\n",
            "📊 RMSE (TFLite Model): 0.3137\n",
            "📊 Precision@5: 0.6000\n",
            "📊 Recall@5: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Function to Descale Predictions\n",
        "def manual_minmax_descale(scaled_ratings, original_min=1, original_max=10):\n",
        "    return scaled_ratings * (original_max - original_min) + original_min\n",
        "\n",
        "# Function to Calculate RMSE\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Function to Calculate Precision@K\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Precision@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]  # Get top-k indices\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()  # Count relevant hits\n",
        "    return relevant / k\n",
        "\n",
        "# Function to Calculate Recall@K\n",
        "def recall_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Recall@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]\n",
        "    total_relevant = np.sum(y_true)\n",
        "    if total_relevant == 0:\n",
        "        return 0.0  # Avoid division by zero\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()\n",
        "    return relevant / total_relevant\n",
        "\n",
        "# Predict on the Test Set\n",
        "predictions = top_n_user_model.predict([user_test, X_test]).flatten()\n",
        "\n",
        "# # Descale Predictions Back to Original Rating Scale (1-10)\n",
        "# descaled_predictions = manual_minmax_descale(predictions)\n",
        "\n",
        "# Calculate RMSE\n",
        "# Transform scaled y_test back to original ratings\n",
        "y_test_original = y_test.reshape(-1, 1).flatten()\n",
        "rmse_score = calculate_rmse(y_test_original, predictions)\n",
        "print(f\"RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Convert True Ratings to Binary Relevance (ratings >= 4.0 are relevant)\n",
        "y_true_binary = (y_test_original >= 0.4).astype(int)\n",
        "\n",
        "# Calculate Precision@K and Recall@K for K=10\n",
        "k = 10\n",
        "k = min(k, len(y_true_binary))  # Adjust if fewer samples are present\n",
        "\n",
        "precision = precision_at_k(y_true_binary, predictions, k)\n",
        "recall = recall_at_k(y_true_binary, predictions, k)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\"Precision@{k}: {precision:.4f}\")\n",
        "print(f\"Recall@{k}: {recall:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zus7jsQ9lKB3",
        "outputId": "dc126a5a-cb11-473e-91bc-0dccc57f945e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "RMSE: 0.2535\n",
            "Precision@10: 0.7000\n",
            "Recall@10: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Do These Metrics Indicate?\n",
        "RMSE = 0.2635 (Scaled)\n",
        "\n",
        "When converted back to the original rating scale:\n",
        "0.2635\n",
        "×\n",
        "(\n",
        "10\n",
        "−\n",
        "1\n",
        ")\n",
        "≈\n",
        "2.37\n",
        "0.2635×(10−1)≈2.37\n",
        "An RMSE of ~2.37 is acceptable for a movie recommendation system, though there’s still room for improvement.\n",
        "\n",
        "Precision@10 = 0.8000\n",
        "\n",
        "Out of the top 10 recommended users, 8 out of 10 were relevant (ratings ≥ 4.0).\n",
        "This suggests the model is good at identifying highly relevant users for a given movie.\n",
        "\n",
        "Recall@10 = 0.0038\n",
        "\n",
        "The model is retrieving only 0.38% of all relevant users.\n",
        "This indicates that while the recommendations are accurate (high precision), the model is missing most relevant users (low recall).\n",
        "\n",
        "Why Is Recall So Low?\n",
        "Imbalanced Data: There could be far more irrelevant users than relevant ones.\n",
        "Top-10 Limitation: You might have many relevant users, but you're only retrieving 10 recommendations.\n",
        "Overfitting on Specific Features: The model might be overfitting on dominant features and missing diverse user preferences.\n",
        "🔧 How to Improve Recall Without Sacrificing Precision\n",
        "✅ Increase K Value\n",
        "\n",
        "Evaluate for higher K values (e.g., 20, 50) to see if recall improves.\n",
        "✅ Add Regularization\n",
        "\n",
        "Introduce L2 regularization to avoid overfitting.\n",
        "✅ Incorporate More User Features\n",
        "\n",
        "Add additional user metadata like watch frequency, historical preferences, or interaction history.\n",
        "✅ Weighted Loss Function\n",
        "\n",
        "Penalize missing relevant users more heavily.\n",
        "✅ Train Longer with Early Stopping\n",
        "\n",
        "Increase the number of training epochs with early stopping to avoid overfitting.\n",
        "Visualize Precision@K and Recall@K for Different K Values\n",
        "Generate a plot to show how Precision@K and Recall@K change for different values of K (e.g., 5, 10, 20, 50)? This can help assess the ideal number of recommendations to provide for users."
      ],
      "metadata": {
        "id": "ribhkJlp4PJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Generate binary encodings for genres, writers, and directors\n",
        "def get_movie_context(fav_genres, fav_writers, fav_directors):\n",
        "    # Encode genres, writers, and directors using MultiLabelBinarizer\n",
        "    genre_vector = mlb_genres.transform([fav_genres]) if fav_genres else np.zeros((1, len(mlb_genres.classes_)))\n",
        "    writer_vector = mlb_writers.transform([fav_writers]) if fav_writers else np.zeros((1, len(mlb_writers.classes_)))\n",
        "    director_vector = mlb_directors.transform([fav_directors]) if fav_directors else np.zeros((1, len(mlb_directors.classes_)))\n",
        "\n",
        "    # Generate dummy numerical features (imdb_rating and numVotes)\n",
        "    dummy_numerical_features = np.zeros((1, 2))  # Assuming these were part of the original features\n",
        "\n",
        "    # Generate dummy user review embeddings\n",
        "    embedding_dim = X_train.shape[1] - (2 + len(mlb_genres.classes_) + len(mlb_writers.classes_) + len(mlb_directors.classes_))\n",
        "    dummy_user_reviews = np.zeros((1, embedding_dim))\n",
        "\n",
        "    # Concatenate all features to match the expected input size\n",
        "    context_vector = np.concatenate([\n",
        "        dummy_user_reviews,\n",
        "        dummy_numerical_features,\n",
        "        genre_vector,\n",
        "        writer_vector,\n",
        "        director_vector\n",
        "    ], axis=1)\n",
        "\n",
        "    return context_vector\n",
        "\n",
        "# ✅ Create a mapping from user_id_encoded to user_id\n",
        "user_id_to_original = dict(zip(range(len(user_encoder.classes_)), user_encoder.classes_))\n",
        "\n",
        "\n",
        "# ✅ Function to recommend users for a movie based on context features\n",
        "def recommend_top_n_users(fav_genres=None, fav_writers=None, fav_directors=None, top_n=10):\n",
        "    # Generate movie context features\n",
        "    movie_context = get_movie_context(fav_genres, fav_writers, fav_directors)\n",
        "\n",
        "    # Prepare inputs for all users\n",
        "    num_users = len(user_encoder.classes_)\n",
        "    user_ids = np.arange(num_users).astype(np.int32).reshape(-1, 1)\n",
        "    movie_context_repeated = np.tile(movie_context, (num_users, 1))\n",
        "\n",
        "    # Predict ratings using the trained model\n",
        "    predictions = top_n_user_model.predict([user_ids, movie_context_repeated])\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "    # Descend predictions back to the original rating scale (1-10)\n",
        "    descaled_predictions = manual_minmax_descale(predictions)\n",
        "\n",
        "    # Get top N user indices\n",
        "    top_indices = descaled_predictions.argsort()[-top_n:][::-1]\n",
        "\n",
        "    # Directly use integer user IDs to access the mapping\n",
        "    recommended_users = []\n",
        "    for i in top_indices:\n",
        "        if i in user_id_to_original:\n",
        "            original_user_id = user_id_to_original[i]\n",
        "            recommended_users.append((original_user_id, descaled_predictions[i]))\n",
        "        else:\n",
        "            print(f\"Warning: Encoded User ID {i} not found in mapping.\")\n",
        "\n",
        "    return recommended_users\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dNlvrzbq5mGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommend top 5 users for a movie with provided genres, writers, and directors\n",
        "top_users = recommend_top_n_users(\n",
        "    fav_genres=['Drama', 'Thriller'],\n",
        "    fav_writers=['nm0522871'],  # IMDb writer IDs\n",
        "    fav_directors=['nm0412650'],  # IMDb director IDs\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "# ✅ Print the top recommended users\n",
        "print(\"Top 5 Users Likely to Enjoy This Movie:\")\n",
        "for user_id, score in top_users:\n",
        "    print(f\"User ID: {user_id}, Predicted Rating: {score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmrr3qTs5qVc",
        "outputId": "2926ce10-3f39-4057-b338-7715d3174f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "Top 5 Users Likely to Enjoy This Movie:\n",
            "User ID: ur1903440, Predicted Rating: 10.16\n",
            "User ID: ur2728323, Predicted Rating: 10.14\n",
            "User ID: ur43988575, Predicted Rating: 10.13\n",
            "User ID: ur2250153, Predicted Rating: 10.06\n",
            "User ID: ur5921551, Predicted Rating: 9.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk model to train model 10k rows."
      ],
      "metadata": {
        "id": "NuKOLBfKKuWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_csv(merged_df_loc, index_col=0).head(20000)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, MinMaxScaler\n",
        "import ast\n",
        "\n",
        "# Safely convert string lists to Python lists using ast.literal_eval\n",
        "def safe_eval_list(x):\n",
        "    if isinstance(x, str):\n",
        "        return ast.literal_eval(x)\n",
        "    return x\n",
        "\n",
        "# Convert string lists into proper lists\n",
        "merged_df['genres'] = merged_df['genres'].apply(safe_eval_list)\n",
        "merged_df['directors_list'] = merged_df['directors_list'].apply(safe_eval_list)\n",
        "merged_df['writers_list'] = merged_df['writers_list'].apply(safe_eval_list)\n",
        "\n",
        "# Convert string representations of embeddings to numpy arrays\n",
        "def convert_string_to_array(x):\n",
        "    if isinstance(x, str):\n",
        "        return np.fromstring(x.strip('[]'), sep=' ')\n",
        "    return np.array(x)\n",
        "\n",
        "# Apply conversion across the DataFrame\n",
        "merged_df['user_reviews_padded'] = merged_df['user_reviews_padded'].apply(convert_string_to_array)\n",
        "\n",
        "# Encode categorical variables\n",
        "mlb_genres = MultiLabelBinarizer()\n",
        "mlb_writers = MultiLabelBinarizer()\n",
        "mlb_directors = MultiLabelBinarizer()\n",
        "\n",
        "genres_encoded = mlb_genres.fit_transform(merged_df['genres'])\n",
        "writers_encoded = mlb_writers.fit_transform(merged_df['writers_list'])\n",
        "directors_encoded = mlb_directors.fit_transform(merged_df['directors_list'])\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler_numeric = MinMaxScaler()\n",
        "numeric_features = scaler_numeric.fit_transform(merged_df[['imdb_rating', 'numVotes']])\n",
        "\n",
        "# Consistent user_reviews_padded embeddings\n",
        "embedding_dim = len(merged_df['user_reviews_padded'].iloc[0])\n",
        "merged_df['user_reviews_padded'] = merged_df['user_reviews_padded'].apply(\n",
        "    lambda x: x if len(x) == embedding_dim else np.zeros(embedding_dim)\n",
        ")\n",
        "\n",
        "# Concatenate all features into X\n",
        "X = np.concatenate([\n",
        "    np.vstack(merged_df['user_reviews_padded'].values),\n",
        "    numeric_features,\n",
        "    genres_encoded,\n",
        "    writers_encoded,\n",
        "    directors_encoded\n",
        "], axis=1)\n",
        "\n",
        "# Define y (scaled user ratings)\n",
        "y = merged_df['user_ratings'].values"
      ],
      "metadata": {
        "id": "fcZmJt8MQ_OB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2aa4aee0-d970-411c-8072-dca2b16bbd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-aba861cb9f5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert string lists into proper lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_eval_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directors_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'directors_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_eval_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'writers_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'writers_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_eval_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-aba861cb9f5b>\u001b[0m in \u001b[0;36msafe_eval_list\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_eval_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \"\"\"\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfeature_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Else it should be an int giving the minor version for 3.x.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     return compile(source, filename, mode, flags,\n\u001b[0m\u001b[1;32m     51\u001b[0m                    _feature_version=feature_version)\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "import gc\n",
        "\n",
        "\n",
        "# Function to manually split X, y, and user IDs into chunks\n",
        "def split_data_into_chunks(X, y, user_ids, chunk_size=10000):\n",
        "    total_size = X.shape[0]\n",
        "    chunks = []\n",
        "    for i in range(0, total_size, chunk_size):\n",
        "        X_chunk = X[i:i + chunk_size]\n",
        "        y_chunk = y[i:i + chunk_size]\n",
        "        user_ids_chunk = user_ids[i:i + chunk_size]\n",
        "        chunks.append(([user_ids_chunk, X_chunk], y_chunk))\n",
        "    return chunks\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_top_n_user_model():\n",
        "    # ✅ User Input and Embedding\n",
        "    user_input = Input(shape=(1,), name='user_input')\n",
        "    user_embedding = Embedding(\n",
        "        input_dim=len(user_encoder.classes_),  # Number of unique users\n",
        "        output_dim=50,  # Embedding size for users\n",
        "        name='user_embedding'\n",
        "    )(user_input)\n",
        "    user_vec = Flatten(name='user_flatten')(user_embedding)\n",
        "\n",
        "    # ✅ Movie Context Input (e.g., genres, writers, directors, user reviews, IMDb ratings)\n",
        "    movie_context_input = Input(shape=(X_train.shape[1],), name='movie_context_input')\n",
        "\n",
        "    # ✅ Concatenate user embeddings with movie context\n",
        "    concat_user_movie = Concatenate()([user_vec, movie_context_input])\n",
        "\n",
        "    # ✅ Dense Layers for Deep Feature Learning\n",
        "    x = Dense(512, activation='relu')(concat_user_movie)\n",
        "    x = Dropout(0.3)(x)  # Dropout layer for regularization\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # ✅ Output Layer: Predict user rating\n",
        "    output = Dense(1, activation='linear')(x)  # Regression output\n",
        "\n",
        "    # ✅ Define and Compile the Model\n",
        "    model = Model(inputs=[user_input, movie_context_input], outputs=output)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse',  # Mean Squared Error for regression\n",
        "        metrics=['mae']  # Mean Absolute Error for evaluation\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into chunks\n",
        "# Verify chunk sizes before training\n",
        "user_encoder = LabelEncoder()\n",
        "merged_df['user_id_encoded'] = user_encoder.fit_transform(merged_df['user_id'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test, user_train, user_test = train_test_split(\n",
        "    X, y, merged_df['user_id_encoded'].values, test_size=0.25, random_state=42\n",
        ")\n",
        "data_chunks = split_data_into_chunks(X_train, y_train, user_train, chunk_size=10000)\n",
        "\n",
        "print(f\"Total training samples: {X_train.shape[0]}\")\n",
        "for idx, (input_data, y_chunk) in enumerate(data_chunks):\n",
        "    print(f\"Chunk {idx + 1}:\")\n",
        "    print(f\"  User IDs shape: {input_data[0].shape}\")\n",
        "    print(f\"  X_chunk shape: {input_data[1].shape}\")\n",
        "    print(f\"  y_chunk shape: {y_chunk.shape}\")\n",
        "\n",
        "# Initialize the model\n",
        "# Train the model incrementally with memory clearing\n",
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "chunk_model = create_top_n_user_model()\n",
        "chunk_count = 0\n",
        "loss_history = []\n",
        "\n",
        "for (input_data, y_chunk) in data_chunks:\n",
        "    user_ids_chunk, X_chunk = input_data\n",
        "\n",
        "    print(f\"Training on chunk {chunk_count + 1}/{len(data_chunks)}\")\n",
        "    print(f\"User IDs shape: {user_ids_chunk.shape}, X_chunk shape: {X_chunk.shape}, y_chunk shape: {y_chunk.shape}\")\n",
        "\n",
        "    # Train the model on the current chunk\n",
        "    history = chunk_model.fit(\n",
        "        [user_ids_chunk, X_chunk],\n",
        "        y_chunk,\n",
        "        epochs=5,\n",
        "        batch_size=16,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Store loss history\n",
        "    loss_history.append(history.history['loss'])\n",
        "\n",
        "    # Clear TensorFlow session and garbage collect memory\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    chunk_count += 1\n",
        "\n",
        "print(\"Loss history for each chunk:\", loss_history)\n",
        "\n"
      ],
      "metadata": {
        "id": "MkyujDyq6uvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Function to Descale Predictions\n",
        "def manual_minmax_descale(scaled_ratings, original_min=1, original_max=10):\n",
        "    return scaled_ratings * (original_max - original_min) + original_min\n",
        "\n",
        "# Function to Calculate RMSE\n",
        "def calculate_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Function to Calculate Precision@K\n",
        "def precision_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Precision@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]  # Get top-k indices\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()  # Count relevant hits\n",
        "    return relevant / k\n",
        "\n",
        "# Function to Calculate Recall@K\n",
        "def recall_at_k(y_true, y_pred, k):\n",
        "    \"\"\"Calculate Recall@K\"\"\"\n",
        "    top_k_indices = np.argsort(y_pred)[-k:][::-1]\n",
        "    total_relevant = np.sum(y_true)\n",
        "    if total_relevant == 0:\n",
        "        return 0.0  # Avoid division by zero\n",
        "    relevant = np.isin(top_k_indices, np.where(y_true == 1)[0]).sum()\n",
        "    return relevant / total_relevant\n",
        "\n",
        "# Predict on the Test Set\n",
        "predictions = chunk_model.predict([user_test, X_test]).flatten()\n",
        "\n",
        "# # Descale Predictions Back to Original Rating Scale (1-10)\n",
        "# descaled_predictions = manual_minmax_descale(predictions)\n",
        "\n",
        "# Calculate RMSE\n",
        "# Transform scaled y_test back to original ratings\n",
        "y_test_original = y_test.reshape(-1, 1).flatten()\n",
        "rmse_score = calculate_rmse(y_test_original, predictions)\n",
        "print(f\"RMSE: {rmse_score:.4f}\")\n",
        "\n",
        "# Convert True Ratings to Binary Relevance (ratings >= 4.0 are relevant)\n",
        "y_true_binary = (y_test_original >= 0.4).astype(int)\n",
        "\n",
        "# Calculate Precision@K and Recall@K for K=10\n",
        "k = 10\n",
        "k = min(k, len(y_true_binary))  # Adjust if fewer samples are present\n",
        "\n",
        "precision = precision_at_k(y_true_binary, predictions, k)\n",
        "recall = recall_at_k(y_true_binary, predictions, k)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\"Precision@{k}: {precision:.4f}\")\n",
        "print(f\"Recall@{k}: {recall:.4f}\")"
      ],
      "metadata": {
        "id": "tX1F9W6FK7gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}